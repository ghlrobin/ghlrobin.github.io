---
title: "[boostcamp AI Tech / Level 1 - AI Math] Day 4 - RNN 첫걸음"
date: 2022-09-22 10:00:00 + 0900
categories: [boostcamp AI Tech, Level 1 / Week 1 - AI Math]
tags: [boostcamp, ai math, level 1, week 1]	# TAG names should always be lowercase
math: true
---
**TOC**
- [Going into the Lecture](#going-into-the-lecture)
- [시퀀스 데이터 다루기](#시퀀스-데이터-다루기)
- [Recurrent Neural Network을 이해하기](#recurrent-neural-network을-이해하기)
- [RNN의 역전파: Backpropagation Through Time(BPTT)](#rnn의-역전파-backpropagation-through-timebptt)
  - [BPTT를 좀 더 살펴보기](#bptt를-좀-더-살펴보기)

---

## Going into the Lecture

* CNN과 다르게 시계열(time series), sequence 데이터에 주로 적용이 되는 network이다
  * 소리, 문자열, 주가 등의 데이터를 스퀀스(sequence) 데이터로 분류한다
* 독립동등분포(i.i.d) 가정을 따르지 않을 때가 많으므로 주의해야하다
  * `개가 사람을 물었다`와 `사람이 개를 물었다`는 다른 의미를 가지고 있다
* 모델 설계는 어렵지 않지만 왜 이렇게 설계를 해야하는지 이해가 필요하다.

- - -
## 시퀀스 데이터 다루기

* 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용한다
  
$$
\begin{align}
P(X_1, ..., X_t) & = P(X_t | X_1, ... X_{t-1})P(X_1, ..., X_{t-1})  \\
& = P(X_t | X_1, ... X_{t-1})P(X_{t-1}|X_1, ..., X_{t-2})P(X_1, ..., X_{t-2}) \\
& = \prod_{s=1}^{t}P(X_s | X_{s-1}, ..., X_1)
\end{align}
$$  

* 과거의 모든 데이터가 필요한 것은 아니다
* 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다
  
$$
\begin{aligned}
X_{t} &\sim P(X_t | X_{t-1}, ..., X_1) \quad\quad \\
X_{t+1} &\sim P(X_{t+1} | X_{t}, X_{t-1}, ..., X_1) 
\end{aligned}
$$

* $X_1$ 까지가 아닌 고정된 길이 $\tau$ 만큼의 시퀀스만 사용하는 경우 $AR(\tau)$ (Autoregressive Model) 자기회귀모델이라고 부른다.
* $\tau$ 는 hyperparameter이다. 즉 모델링하기 전에 우리가 정해 줘야하는 변수이며 문제에 따라서 $\tau$ 가 바뀔 때가 있고 이런 경우에 사용되는 방법이 RNN의 기본 모형인 Latent Autoregressive Model(잠재자기회기 모델)이다.
* 과거 바로 이전의 정보를 제외한 나머지 정보들을 $H_t$ 라는 잠재변수로 인코딩해서 활용하는 잠재 AR 모델이다
  * 기리가 가변적이지 않고 고정된 길이의 데이터를 가지고 모델링을 할 수 있는 장점이 있
  * 과거의 정보들의 잠재 변수를 어떻게 인코딩할지가 선택의 문제.
  * 이를 해결하는게 RNN이다.

![](/assets/img/boostcamp/2022-09-22-15-49-42.png)


- - -
## Recurrent Neural Network을 이해하기

* 가장 기본적인 RNN 모형은 MLP와 유사한 모양이다.

$$
\begin{aligned}
\mathbf{H}_t &= \sigma(\mathbf{X}_t\mathbf{W}^{(1)}_{X} + \mathbf{H}_{t-1}\mathbf{W}^{(1)}_{H} + \mathbf{b}^{(1)}) \\
\mathbf{O} &= \mathbf{HW}^{(2)} + \mathbf{b}^{(2)} \quad\quad\quad\quad\quad\quad\quad \\ 
\end{aligned}
$$  

* 이전 순서의 잠재변수($H_{t-1}$)와 현재의 입력($X_t$)을 활용하여 모델링한다.


- - -
## RNN의 역전파: Backpropagation Through Time(BPTT)

* 빨간색 화살표가 역전파.

![](/assets/img/boostcamp/2022-09-22-18-38-31.png)

* 잠재변수에 두 변수가 들어옴
  1. 다음 시점에서의 잠재변수에서 들어오는 gradient vector
  2. 출력에서 들어오는 gradient vector
* 출력도 2곳이다
  1. 입력
  2. 이전 시점의 잠재변수.

### BPTT를 좀 더 살펴보기

* BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면 미분의 곱으로 이루어진 항이 계산된다.
* 손실함수는 아래와 같다.

$$
L(x, y, w_h, w_o) = \sum_{t = 1}^{T}l(y_t, o_t) \quad\quad h_t = f(x_t, h_{t-1}, w_h) \text{ and } o_t = g(h_t, w_o)\\
$$

* 편미분을 잘하면 아래와 같은 결과가 나온다.

$$
\partial_{w_{h}}L(x, y, w_h, w_o) = \sum_{t=1}^{T}\partial_{w_h}l(y_t, o_t) = \sum_{t=1}^T\partial_{o_t}l(y_t, o_t)\partial_{h_t}g(h_t , w_h)[\partial_{w_h}h_t]\\
$$

$$
\partial_{w_h}h_t = \partial_{w_h}f(x_t, h_{t-1}, w_h) + \sum_{i=1}^{t-1}\left( \prod_{j=i+1}^{t} \partial_{h_{j-1}}f(x_j, h_{j-1}, w_h)\partial_{w_h}f(x_i, h{i-1}, w_h) \right)
$$  

* 현재 시점부터 예측이 끝나는 $t$시점까지 시퀀스의 길이가 길어질수록 $\prod_{j=i+1}^{t} \partial_{h_{j-1}}f(x_j, h_{j-1}, w_h)$은 불안정해지기 쉽다.
  * 1 보다 크면 폭발, 1 보다 작으면 기울기가 되게 작어진다
  * 작아질 때를 **기울기 소실**(vanishing gradient)이라고 한다
* 기울기 소실를 해결하기 위해 **truncated BPTT**를 사용한다
  * 시퀀스 길이를 끊는 것
  * 하지만 완벽한 해결책은 아니다
  * 그래서 LSTM이나 GRU가 등장했다.

- - -