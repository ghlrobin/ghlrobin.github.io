---
title: "[boostcamp AI Tech][PyTorch] Lecture 7: Monitoring Tools for PyTorch"
date: 2022-09-29 12:00:00 + 0900
categories: [boostcamp AI Tech, Week 2 - PyTorch]
tags: [boostcamp, pytorch, level 1, week 2] # TAG names should always be lowercase
math: true
---

- [Going into the Lecture](#going-into-the-lecture)
- [Tensorboard](#tensorboard)
- [weight & biases](#weight--biases)

# Going into the Lecture

- 학습 시간이 길기 때문에 기다림의 기록이 필요한데 좋은 도구들이 많다
- **Tensorboard**와 **weight & biases**라는 도구가 있다

# Tensorboard

- TensorFlow의 프로젝트로 만들어진 시각화 도구이다
- 학습 그래프, metric, 학습 결과의 시각화를 지원한다
- PyTorch도 연결이 가능하며 DL 시각화하는데 핵심 도구이다
- scalar: metric등 상수 값의 연속(epoch)을 표시
- graph: 모델의 computational graph 표스
- histogram: weight등 값의 분포를 표현
- image: 예측 값과 실제 값을 비교해서 표시
- mesh: 3D 형태의 데이터를 표현하는 도구

```python
import os
logs_base_dir = "logs"
os.makedirs(logs_base_dir, exist_ok = True) # Tensorboard 기록을 위한 directory 생성

from torch.utils.tensorboard import SummaryWriter # 기록 생성 객체 SummaryWriter 생성
import numpy as np

writer = SummaryWriter(logs_base_dir) # 어디에 기록해야 되는지 설정
for n_iter in range(100):
        writer.add_scalar('Loss/train', np.random.random(), n_iter) # add_scalar: scalar 값을 기록
        writer.add_scalar('Loss/test', np.random.random(), n_iter) # Loss/train: loss category에 train 값
        writer.add_scalar('Accuracy/train', np.random.random(), n_iter) # n_iter: x 축의 값
        writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
writer.flush() # 값을 disk에 쓰기(기록하기)

%load_ext tensorboard #jupter 상에서 tensorboard 수행
%tensorboard --logdir{logs_base_dir} # 파일 위치 지정. 같은 명령어를 콘솔에서도 사용 가능하다
```

# weight & biases

- 머신러닝 실험을 원활히 지원하기 위한 상용도구이다
- 협업, code versioning, 실험 결과 기록 등을 제공한다
- MLOps의 대표적인 툴로 저변 확대 중이다.

```python
!pip instal wandb -q 
import wandb
wandb.init(project="my_project_name", entity="user_name") #wandb 연결
```
```python
EPOCHS = 100
BATCH_SIZE = 64
LEARNING_RATE = 0.001

config={"epochs": EPOCHS, "batch_size": BATCH_SIZE, "learning_rate" : LEARNING_RATE}

wandb.init(project="my_project_name", config=config)
# wandb.config.batch_size = BATCH_SIZE
# wandb.config.learning_rate = LEARNING_RATE
# config={"epochs": EPOCHS, "batch_size": BATCH_SIZE, "learning_rate" : LEARNING_RATE}

for e in range(1, EPOCHS+1):
    '''
    your train valid code
    '''
        
    train_loss = epoch_loss/len(train_dataset)
    train_acc = epoch_acc/len(train_dataset)
    print(f'Epoch {e+0:03}: | Loss: {train_loss:.5f} | Acc: {train_acc:.3f}')
    wandb.log({'accuracy': train_acc, 'loss': train_loss})
```

- 자신의 훈련을 위한 모델의 parameter를 config로 설정해서 전달해준다
- 해당 모델의 훈련값이 wandb.log 코드를 통해 wandb에 전달되어 기록된다.







-----------------------------------


