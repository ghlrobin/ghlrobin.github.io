---
title: "[boostcamp AI Tech][DL Basic] Lecture 5: Modern Convolutional Neural Networks"
date: 2022-10-03 13:00:00 + 0900
categories: [boostcamp AI Tech, Week 3 - DL Basic]
tags: [boostcamp, dl basic, level 1, week 3] # TAG names should always be lowercase
math: true
---

- [Going into the Lecture](#going-into-the-lecture)
- [ILSVRC](#ilsvrc)
- [AlexNet(2012)](#alexnet2012)
- [VGGNet(2014)](#vggnet2014)
- [GoogLeNet(2014)](#googlenet2014)
- [ResNet(2015)](#resnet2015)
- [DenseNet(2017)](#densenet2017)
- [Summary](#summary)

# Going into the Lecture

- We have a look at five ImageNet winning CNN architectures to understand their core ideas and structures.
1. AlexNet (2012) - first DL architecture to win ILSVRC
2. VGG (2014) - Very deep convolutional network that uses smaller size filters (3x3) aimed to reduce the # of parameters
3. GoogLeNet (2014) - Introduced inception blocks
4. ResNet (2015) - Introduced residual block to reduce overfitting
5. DenseNet(2017) - Similar in idea to ResNet but used Concatenation instead of Addition.

# ILSVRC

- ImageNet Large-Scale Visual Recognition Challenge
- Tests Classification / Detection / Localization / Segmentation
- 1000 different categories
- Over 1 million images
- Training set: 456,567 images and counting.

![](/assets/img/boostcamp/2022-10-03-22-19-53.png)

- Considering that human average performance is lower than 2015's winning architecture's, it is a little meaningless.

# AlexNet(2012)

![](/assets/img/boostcamp/2022-10-03-22-24-40.png)

- The input to the network is a batch of RGB images of size 227x227x3 and outputs a 1000x1 probability vector one correspoding to each class
- One of the first deep CNN to achieve considerable accuracy with an accuracy of 84.7% on the 2012 ILSVRC challenge
- The network consists of 5 CONV layers and 3 FC layers
- The activation used is ReLU (before AlexNet it was sigmoid and tanh)
  - Preserves properties of linear models
  - Easy to optimize with gradient descent
  - Good generalizationo
  - Overcome the vanishing gradient problem
  - Although ReLU helps with the vanishing gradient problem, due to its unbounded nature, the learned variables can become unnecessarily high. To prevent this, AlexNet introduced Local Response Normalization (LRN)
- Used 2 GPUs
- Data augmentation and dropout is carried out to reduce over-fitting. (Mirroring, cropping)
- The network uses an overlapped max-pooling layer after the 1st, 2nd and 5th CONV layers
  - Overlapped maxpool layers are simply maxpool layers with strides less than the window size. 3x3 maxpool layer is used with a stride of 2 hence creating overlapped receptive fields. This overlapping improved the top-1 and top-5 errors by 0.4% and 0.3%, respectively.

# VGGNet(2014)

![](/assets/img/boostcamp/2022-10-03-22-36-04.png)

- Why? Born out of the need to reduce the # of parameters in the CONV layers and improve on training time
- There are multiple variants of VGGNet (VGG16, VGG19, etc.) which differ only in the total number of layers in the network (16 layers, 19 layers, etc)
- All the conv kernels are of size 3x3 and maxpool kernels are of size 2x2 with a stride of two
  - The idea behind having fixed size kernels is that all the variable size convolutional kernels used in Alexnet (11x11, 5x5, 3x3) can be replicated by making use of multiple 3x3 kernels as building blocks. The replication is in terms of the receptive field covered by the kernels
  - 5x5 kernel can be replicated by two 3x3 kernels.

![](/assets/img/boostcamp/2022-10-03-22-40-27.png)

- As you can see having two 3x3 filter CONV layers is much better than one 5x5 filter CONV layer
- Similarly, the effect of one 7x7 (11x11) conv layer can be achieved by implementing three (five) 3x3 conv layers with a stride of one. This reduces the number of trainable variables by 44.9% (62.8%).

# GoogLeNet(2014)

- Won the ILSVRC at 2014
  - combined network-in-network (NiN) with inception blocks
  - 22 layers
- 1x1 convolutions used effectively
- In an image classification task, the size of the salient feature can considerably vary within the image frame. Hence, deciding on a fixed kernel size is rather difficult. Lager kernels are preferred for more global features that are distributed over a large area of the image, on the other hand, smaller kernels provide good results in detecting area-specific features that are distributed across the image frame. For effective recognition of such a variable-sized feature, we need kernels of different sizes. That is what Inception does. Instead of simply going deeper in terms of the number of layers, it goes wider. Multiple kernels of different sizes are implemented within the same layer
- The Inception network architecture consists of several inception moduels of the following structure.

![](/assets/img/boostcamp/2022-10-03-22-46-25.png)

- Each inception modules consists of four operations in parallel
1. 1x1 conv layer: for depth reduction (to reduce the number of parameters)
2. 3x3 conv layer: captures distributed features
3. 5x5 conv layer: captures global features
4. max pooling: captures low level features that stand out in a neighborhood.

- all of these features are extracted and concatenated before it is fed to the next layer

- below shows the benefit of 1x1 convolutions.

![](/assets/img/boostcamp/2022-10-03-22-52-58.png)

![](/assets/img/boostcamp/2022-10-03-22-53-43.png)

# ResNet(2015)

- Deeper neural networks are hard to train
  - Overfitting is usually caused by an excessive number of parameters
- ResNet adds an identity map/shortcut (skip connection) to solve the vanishing gradient problem exacerbated by deep networks.
  
![](/assets/img/boostcamp/2022-10-03-22-58-20.png)

- In the case where the channel depth doesn't match, ResNet uses projected shortcut that uses 1x1 conv to match the channel depth.

![](/assets/img/boostcamp/2022-10-03-23-03-13.png)

- interestingly batch norm is done after 3x3 convolution

- bottleneck architecture
  - uses 1x1 conv to reduce input channel and then increase it back to match whatever channel we want.

![](/assets/img/boostcamp/2022-10-03-23-05-41.png)

- Performance increase while paramter size decreases.

![](/assets/img/boostcamp/2022-10-03-23-06-27.png)

# DenseNet(2017)

- DenseNet usese conactenation instead of addition
  - This means that the data doesn't mix.

![](/assets/img/boostcamp/2022-10-03-23-07-24.png)

- Dense Block
  - Each layer concatenates the feature maps of all preceding layers
  - The number of channels increases geometrically
- Transition Block
  - BatchNorm -> 1x1 conv -> 2x2 AvgPooling
  - This reduces the number of channels that were increased by Dense Blocks.

![](/assets/img/boostcamp/2022-10-03-23-09-52.png)

# Summary

- Key Takeaways
  - AlexNet: ReLu, Data augmentation
  - VGG: repeated 3x3 blocks
  - GoogLeNet: 1x1 convolution and inception modules
  - ResNet: skip connection
  - DenseNet: concatenation.

[reference](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96)

-------------------------------


