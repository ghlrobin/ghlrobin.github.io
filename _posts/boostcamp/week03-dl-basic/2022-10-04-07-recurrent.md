---
title: "[boostcamp AI Tech][DL Basic] Lecture 7: Recurrent Neural Networks"
date: 2022-10-04 10:00:00 + 0900
categories: [boostcamp AI Tech, Week 3 - DL Basic]
tags: [boostcamp, dl basic, level 1, week 3] # TAG names should always be lowercase
math: true
---

- [Sequential Model](#sequential-model)
  - [Autoregressive Model](#autoregressive-model)
  - [Markov Model (first-order autoregressive model)](#markov-model-first-order-autoregressive-model)
  - [Latent Autogressive Model](#latent-autogressive-model)
- [Recurrent Neural Network](#recurrent-neural-network)
- [Long Short Term Network (LSTM)](#long-short-term-network-lstm)
  - [Forget Gate](#forget-gate)
  - [Input Gate](#input-gate)
  - [Update Cell](#update-cell)
  - [Output Gate](#output-gate)
- [Gated Recurrent Unit (GRU)](#gated-recurrent-unit-gru)

# Sequential Model

- Sequential data: audio, video, motion
  - the biggest challenge is that we don't know when the data ends - we don't know the dimension of the input data
- Naive sequence model.

## Autoregressive Model

![](/assets/img/boostcamp/2022-10-04-09-47-38.png)

- This model fixes the past timespan so that we only check $\tau$ past data.

## Markov Model (first-order autoregressive model)

![](/assets/img/boostcamp/2022-10-04-09-51-11.png)

- This is the easiest representation of an autoregressive model
- Only dependent on the step just before
- Quite ridiculous as it throws away too much infomation
- But it's very easy to represent.

## Latent Autogressive Model

![](/assets/img/boostcamp/2022-10-04-09-52-51.png)

- This model adds a hidden latent state, $h_t$, which acts as a summary of the past.

# Recurrent Neural Network

![](/assets/img/boostcamp/2022-10-04-10-14-07.png)

- If you spread it out with respect to time, you get the image on the right.

![](/assets/img/boostcamp/2022-10-04-10-17-01.png)

- the biggest problem is short-term dependencies
  - the further away the past data, the weaker it is
  - this is caused by vanishing gradient (if activation function is sigmoid) or exploding gradient (if activation function is ReLU).

# Long Short Term Network (LSTM)

![](/assets/img/boostcamp/2022-10-04-10-23-30.png)

- This moodel was created to solve the short-term dependency problem of RNN
- $x_t$ = input
- $h_{t-1}$ = previous hiddent state and $h_t$ = output(hidden state)
- $C_{t-1}$ = previous cell state and $C_t$ = next cell state.

There are three gates $\sigma$ in order:
1. forget gate
2. input gate
3. output gate.

## Forget Gate

![](/assets/img/boostcamp/2022-10-04-10-26-53.png)

- Forget gate decides which information to **throw** away.

## Input Gate

![](/assets/img/boostcamp/2022-10-04-10-28-50.png)

- Input gate decides which information to **store** in the cell state.

## Update Cell

![](/assets/img/boostcamp/2022-10-04-10-31-53.png)

- Update the cell state.

## Output Gate

![](/assets/img/boostcamp/2022-10-04-10-32-45.png)

- Make output using the update cell state.

# Gated Recurrent Unit (GRU)

![](/assets/img/boostcamp/2022-10-04-10-35-06.png)

- It's an altered LSTM model
- It's a simpler architecture with two gates (**reset gate** and **update gate**)
- No **cell state**, just **hidden state**
- GRU has less parameters than LSTM
- In truth, both GRU and LSTM are not really used nowadays since they were replaced by a better performing model, Transformers.

[reference](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

-------------------------------


