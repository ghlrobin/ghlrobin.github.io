---
title: "[boostcamp AI Tech][DL Basic] Lecture 9: Generative Models Part 1"
date: 2022-10-04 13:00:00 + 0900
categories: [boostcamp AI Tech, Week 3 - DL Basic]
tags: [boostcamp, dl basic, level 1, week 3] # TAG names should always be lowercase
math: true
---

- [Transformer](#transformer)

# Learning a Generative Model

- Suppose that we are given images of dogs
- We want to learn a probability distribution $p(x)$ such that
  - Generation: if we sample $\tilde{x} \sim p(x)$, $\tilde{x}$ should look like a dog
  - Density estimation: $p(x)$ should be high if $x$ looks like a dog, and low otherwise
    - This is also known as explicit models
  - Then, how can we represent $p(x)$?


# Example

- When modeling a single pixel of an RGB image we have $(r,g,b) \sim p(R,G,B)$. The number of possible cases is 256 x 256 x 256 but the parameters we need to specify is 256 x 256 x 256 - 1. We need one less because the last number is dependent upon the rest.

# Independence

![](/assets/img/boostcamp/2022-10-04-21-18-36.png)

- Suppose we have $X_1, ..., X_n$ of $n$ binary pixels (black or white). then the number of cases is $2^n$ and the parameters we need is $2^n - 1$
- This shows that it's impossible to even model a basic image because the number of parameters is so large.
- But if $X_1, ..., X_n$ are independent, then by independence, $P(X_1,...,X_n) = P(X_1)P(X_2)...P(X_n)$
- Then the number of cases is still $2^n$ but the number of parameters is simply $n$
- However, this doesn't mean much if not, anything. So we want to find a middle ground, that is *Conditional Independence*

# Conditional Independence

- There are three important rules
  1. Chain rule: $p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_2,x_1)...p(x_n|x_{n-1},...,x_1)$
  2. Bayes' rule: $p(x|y) = \frac{p(x,y)}{p(y)}$
  3. Condtional Independence: if $x \perp y|z$, then $p(x|y,z) = p(x|z)$

Then using the chain rule $P(x_1,...,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_2,x_1)...P(x_n|x_{n-1},...,x_1)$. Then calculating the number of parameters...
- $P(X_1)$: 1 parameters
- $P(X_2|X_1)$: 2 parameters (one per $X_1 = 0, 1$)
- $P(X_3|X_1, X_2)$: 2 parameters
- Hence the total is $1 + 2 + 2^2 + ... + 2^{n-1} = 2^n - 1$ which is the same as before

Now applying the Markov assumption, we get $p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_2)...p(x_n|x_{n-1})$ which has $2n-1$ parameters
- By leveraging the Markov assumption, we get exponential reduction on the number of parameters
- Autoregressive models leverge this conditional independency.

# Autoregressive Model (AR Model)

- Suppose we have 28 x 28 binary pixels.
- Our goal is to learn $P(X) = P(X_1,...,X_{789})$ over $X \in {0, 1}^{784}$ 
- Then, how can we parametrize $P(X)$?
  - We use the chain rule to factor the joint distribution
  - $P(X_{1:784} = P(X_1)P(X_2|X_1)P(X_3|X_2)...$
  - This is called an autoregressive model
  - Note that we need an ordering (e.g. raster scan order) of all random variables.

# NADE: Neural Autoregressive Density Estimator

- NADE is an explicit model that can compute the density of the given inputs
- How can we compute the density of the given image?
  - Suppose that we have a binary image with 784 binary pixels
  - Then, the joint probability is computed by $p(x_{1:784} = p(x_1)p(x_2|X_1)p(x_3|X_2)...p(x_784|x_{1:783})$
- In case of modeling continuous random variables, a mixture of Gaussian (MoG) can be used.

# Summary of AR Models

- Easy to sample from
- Easy to compute probability
- Easy to extend to continuous variables.
















-------------------------------