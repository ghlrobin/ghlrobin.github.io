<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://ghlrobin.github.io/</id><title>Coding Fish</title><subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle> <updated>2022-10-05T11:11:56+09:00</updated> <author> <name>Coding Fish</name> <uri>https://ghlrobin.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://ghlrobin.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://ghlrobin.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2022 Coding Fish </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>[boostcamp AI Tech][Data Viz] Lecture 01: Introduction to Visualization</title><link href="https://ghlrobin.github.io/posts/01-introduction-to-visualization/" rel="alternate" type="text/html" title="[boostcamp AI Tech][Data Viz] Lecture 01: Introduction to Visualization" /><published>2022-10-05T10:00:00+09:00</published> <updated>2022-10-05T10:00:00+09:00</updated> <id>https://ghlrobin.github.io/posts/01-introduction-to-visualization/</id> <content src="https://ghlrobin.github.io/posts/01-introduction-to-visualization/" /> <author> <name>Coding Fish</name> </author> <category term="boostcamp AI Tech" /> <category term="Week 3 - Data Visualization" /> <summary> 데이터 시각화란 무엇일까? 데이터 시각화란 무엇일까? 다양한 요소가 포함된 작업이다 목적: 왜 시각화를 하나요? 독자: 시각화 결과는 누구를 대상으로 하나요? 데이터: 어떤 데이터를 시각화할 것인가요? 스토리: 어떤 흐름으로 인사이트를 전달할 것인가요? 방법: 전달하고자 하는 내용에 맞게 효과적인 방법을 사용하고 있나요? 디자인: UI에서 만족스러운 디자인을 가지고 있나요? 데이터 이해하기 데이터 시각화를 위해서는 데이터가 우선적으로 필요하고 시각화를 진행할 데이터를 데이터셋 관점(global)에서 또는 개별 데이터의 관점(local)에서 정리할지를 정해야 한다. 데이터셋의 종류 요즘에는 수많은 데이터셋이 존재를 한다. 정형 데... </summary> </entry> <entry><title>[boostcamp AI Tech][Day 12] Week 3 Sleepy</title><link href="https://ghlrobin.github.io/posts/day-12/" rel="alternate" type="text/html" title="[boostcamp AI Tech][Day 12] Week 3 Sleepy" /><published>2022-10-04T23:00:00+09:00</published> <updated>2022-10-04T23:00:00+09:00</updated> <id>https://ghlrobin.github.io/posts/day-12/</id> <content src="https://ghlrobin.github.io/posts/day-12/" /> <author> <name>Coding Fish</name> </author> <category term="boostcamp AI Tech" /> <category term="Day in Review" /> <summary> Day in Review It’s up to me how to best use the resources around me and I have to act as such. Whether it be time provided for mentor session, the time I have to study and the time I have to rest. I guess a lot of it is to do with time. And it seems right now that I don’t have enough of it. My To-Do list is getting full while I can only do so much. Again, I just need to keep my head down and... </summary> </entry> <entry><title>[boostcamp AI Tech][DL Basic] Lecture 10: Generative Models Part 2</title><link href="https://ghlrobin.github.io/posts/10-generative-2/" rel="alternate" type="text/html" title="[boostcamp AI Tech][DL Basic] Lecture 10: Generative Models Part 2" /><published>2022-10-04T22:00:00+09:00</published> <updated>2022-10-04T22:00:00+09:00</updated> <id>https://ghlrobin.github.io/posts/10-generative-2/</id> <content src="https://ghlrobin.github.io/posts/10-generative-2/" /> <author> <name>Coding Fish</name> </author> <category term="boostcamp AI Tech" /> <category term="Week 3 - DL Basic" /> <summary> Maximum Likelihood Learning Maximum Likelihood Learning Given a training set of examples, we can cast the generative model learning process as finding the best-approximating density model from the model family Then, how can we evalutate the goodness of the approximation? </summary> </entry> <entry><title>[boostcamp AI Tech][DL Basic] Lecture 9: Generative Models Part 1</title><link href="https://ghlrobin.github.io/posts/09-generative-1/" rel="alternate" type="text/html" title="[boostcamp AI Tech][DL Basic] Lecture 9: Generative Models Part 1" /><published>2022-10-04T13:00:00+09:00</published> <updated>2022-10-04T21:57:59+09:00</updated> <id>https://ghlrobin.github.io/posts/09-generative-1/</id> <content src="https://ghlrobin.github.io/posts/09-generative-1/" /> <author> <name>Coding Fish</name> </author> <category term="boostcamp AI Tech" /> <category term="Week 3 - DL Basic" /> <summary> Learning a Generative Model Example Independence Conditional Independence Autoregressive Model (AR Model) NADE: Neural Autoregressive Density Estimator Summary of AR Models Learning a Generative Model Suppose that we are given images of dogs We want to learn a probability distribution $p(x)$ such that Generation: if we sample $\tilde{x} \sim p(x)$, $\tilde{x}$ ... </summary> </entry> <entry><title>[boostcamp AI Tech][DL Basic] Lecture 8: Transformer</title><link href="https://ghlrobin.github.io/posts/08-transformer/" rel="alternate" type="text/html" title="[boostcamp AI Tech][DL Basic] Lecture 8: Transformer" /><published>2022-10-04T11:00:00+09:00</published> <updated>2022-10-04T12:24:41+09:00</updated> <id>https://ghlrobin.github.io/posts/08-transformer/</id> <content src="https://ghlrobin.github.io/posts/08-transformer/" /> <author> <name>Coding Fish</name> </author> <category term="boostcamp AI Tech" /> <category term="Week 3 - DL Basic" /> <summary> Sequential Model Transformer Transformer is the first sequence transduction model based entirely on attention Proccesses sequential data and encodes. Not only works for NLP but for visual transformer, text-to-image (DALL-E) Things we need to understand: Given n words how are they processed into the encoder? What is the information being sent from encoders to decoders? How... </summary> </entry> </feed>
